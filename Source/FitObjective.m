function [m, con, G, D, opts] = FitObjective(m, con, obj, opts)
%FitObjective Optimize the parameters of a model to minimize a set of
%   objective functions
%
% Usage:
%   1. Original way, 1st set of input args below. Single model with a
%      vector of conditions and matrix of objective functions, where all
%      conditions are derived from the model, and each row of objective
%      functions is derived from the corresponding condition row.
%	2. New way, 2nd set of input args below. Multiple models,
%      conditions, and objective function vectors, plus an options struct
%      generated by BuildFitOpts to create the desired mapping.
%
%  Mathematically: T = argmin(G(T))
%
% FitObjective uses the derivatives in the Kronecker model and in the
% objective functions to build a function that can not only evaluate the
% objective functions at particular parameter sets, but also evaluate the
% gradient at those parameter sets, thereby pointing in the direction of
% a more optimum parameter set. This function is built around Matlab's
% fmincon, which is a gradient descent minimizer. It varies the
% parameters attempting to find the parameter set that will minimize the
% objective function while keeping with the bounds.
%
% Inputs:
%	m [ model struct scalar | nModel x 1 model struct vector ]
%       The KroneckerBio model that will be simulated
% 	con [ nCon x 1 experiment struct vector | nCon x 1 experiment struct vector ]
%       The experimental conditions under which the model will be simulated
%	obj [ nCon x nCon objective struct matrix | nObj x 1 objective struct vector ]
%       The objective structures defining the objective functions to be
%       evaluated.
%	opts [ options struct scalar ]
%       Options struct with just global opts or detailed options from
%       BuildFitOpts.
%
% Outputs:
%	m [ model struct scalar | nModel x 1 model struct vector ]
%       The model(s) with optimal rate parameters applied. If fitting parameters
%       that vary betwen experimental conditions, extra models are returned
%       (correspondence between m and con are in opts.fit.ComponentMap) to allow
%       immediate simulation of fits.
%	con [ nCon x 1 experiment struct vector | nCon x 1 experiment struct vector ]
%       The experimental conditions with optimal seed, input control, and
%       dose control parameters applied
%	G [ real scalar ]
%       The optimum objective function value
%	D [ real vector nT ]
%       The objective gradient at the optimum parameter set
%   opts [ options struct scalar ]
%       Options struct that's been validated and updated by FitObjective.
%       In particular, FitObjective may modify opts to add dummy models if
%       different experimental conditions w/ different rate params require it.

%% Work-up
% Clean up inputs
if nargin < 4
    opts = [];
end
if nargin < 4 || ~isfield(opts, 'fit')
    % Convert Usage 1 -> Usage 2
    [m, con, obj, opts] = ConvertFitOpts(m, con, obj, opts);
end

validateOpts(m, con, obj, opts);

verbose = logical(opts.Verbose);
opts.Verbose = max(opts.Verbose-1,0);

% Ensure allowed optimization algorithm is selected
method = opts.Method;
switch method
    case 'fmincon'
        % do nothing
    otherwise
        error('KroneckerBio:FitObjective:InvalidMethod', 'Invalid optimization method %s selected.', method)
end

% Ensure Restart is a positive integer
if ~(opts.Restart >= 0)
    opts.Restart = 0;
    warning('KroneckerBio:FitObjective', 'opts.Restart was not nonegative. It has been set to 0.')
end

if ~(opts.Restart == floor(opts.Restart))
    opts.Restart = floor(opts.Restart);
    warning('KroneckerBio:FitObjective', 'opts.Restart was not a whole number. It has been floored.')
end

% Ensure RestartJump is a function handle
if isnumeric(opts.RestartJump)
    opts.RestartJump = @(iter,G)(opts.RestartJump);
end

% Add dummy models if multiple conditions depend on independent k's
nCon = length(con);
addDummyModel = opts.fit.AddDummyModel;
for iCon = 1:nCon
    iDummy = addDummyModel(iCon);
    if iDummy % not 0
        % See if the required dummy model has already been added
        prevDummy = addDummyModel(1:iCon-1);
        if ismember(iDummy, prevDummy)
            iDummyNew = opts.fit.ComponentMap{ismember(prevDummy, iDummy),1}; % get reassigned model ind from previously added dummy
        else
            iBaseModel = opts.fit.ComponentMap{iCon,1};
            mName = m(iBaseModel).Name;
            mDummy = m(iBaseModel);
            m = [m; mDummy];
            iDummyNew = length(m);
            m(iDummyNew) = m(iDummyNew).UpdateField(struct('Name', [mName '_Dummy_' num2str(iDummyNew)]));
        end
        opts.fit.ComponentMap{iCon,1} = iDummyNew; % reassign component map to point to dummy model
    end
end

validateOpts(m, con, obj, opts);

% Collect bounds
LowerBound = opts.fit.Tlocal2T(opts.fit.LowerBounds);
UpperBound = opts.fit.Tlocal2T(opts.fit.UpperBounds);

% Assign starting values of params (default is values in model when opts was built)
[m, con] = updateAllActiveParameters(m, con, opts.fit.Tlocal2T(opts.fit.ParamStart), opts.fit.ComponentMap, opts.fit.T2Tlocal);

% Construct starting variable parameter set
nT = length(LowerBound);
T0 = collectAllActiveParameters(m, con, opts.fit.ComponentMap, opts.fit.Tlocal2T);
assert(length(T0) == nT);

%% Local optimization options
localOpts = optimoptions('fmincon');
localOpts.Algorithm               = opts.Algorithm;
localOpts.TolFun                  = opts.TolOptim;
localOpts.TolX                    = 0;
localOpts.OutputFcn               = @isTerminalObj;
localOpts.GradObj                 = 'on';
localOpts.Hessian                 = 'off'; % unused
localOpts.MaxFunEvals             = opts.MaxFunEvals;
localOpts.MaxIter                 = opts.MaxIter;
localOpts.RelLineSrchBnd          = opts.MaxStepSize;
localOpts.RelLineSrchBndDuration  = Inf;
localOpts.TolCon                  = 1e-6;

if verbose
    localOpts.Display = 'iter';
else
    localOpts.Display = 'off';
end

%% Global optimization options
% TODO: make sure options are relevant for solver; extend global solver API
globalOpts = opts.GlobalOpts;

% Sanity checking
if strcmpi(globalOpts.Algorithm, 'multistart') && globalOpts.UseParallel
    warning('KroneckerBio:FitObjective', 'Using multistart with UseParallel is not supported at this time (due to global variable in obj fun usage).')
end

%% Normalize parameters
if opts.Normalized
    % Normalize starting parameters and bounds
    T0 = log(T0);
    LowerBound = log(LowerBound);
    UpperBound = log(UpperBound);
    
    % Change relative line search bound to an absolute scale in log space
    % Because fmincon lacks an absolute option, this hack circumvents that
    localOpts.TypicalX = zeros(nT,1) + log(1 + opts.MaxStepSize)*log(realmax);
    localOpts.RelLineSrchBnd = 1 / log(realmax);
end

%% Apply bounds to starting parameters before optimizing
% fmincon will choose a weird value if a starting parameter is outside the bounds
T0(T0 < LowerBound) = LowerBound(T0 < LowerBound);
T0(T0 > UpperBound) = UpperBound(T0 > UpperBound);

%% Abort in rare case of no optimization
if numel(T0) == 0
    [G, D] = fminconObjective(T0);
    return
end

%% Run optimization
% Initialize loop
That = T0;
Gbest = Inf;
Tbest = T0;

%%
% Check for parallel toolbox if parallel optimization is specified
if opts.RunParallelExpts && isempty(ver('distcomp'))
    warning('KroneckerBio:FitObjective:ParallelToolboxMissing', ...
        ['opts.UseParallelExpts requires the Parallel '...
        'Computing toolbox. Reverting to serial evaluation.'])
    opts.RunParallelExpts = false;
end

% Disable experiment parallelization if global optimization is desired
if opts.GlobalOptimization && opts.RunParallelExpts
    warning('KroneckerBio:FitObjective:GlobalOptimizationParallelExperimentsNotSupported', ...
        ['Both opts.GlobalOptimization and opts.RunParallelExpts ' ...
        'were set to true. Disabling parallelized experiments.'])
    opts.RunParallelExpts = false;
end

% Initialize a parallel pool, or get the current one.
if opts.RunParallelExpts
    % Validate opts.RunParannelExpts
    assert(isscalar(opts.RunParallelExpts), 'KroneckerBio:FitObjective:NonscalarRunParallelExptsOpt', 'opts.RunParallelExpts must be a scalar.')
    if islogical(opts.RunParallelExpts)
        nwMax = feature('numCores');
    elseif isnumeric(opts.RunParallelExpts) && opts.RunParallelExpts > 0
        nwMax = min(opts.RunParallelExpts, feature('numCores'));
    else
        error('KroneckerBio:FitObjective:InvalidRunParallelExptsOpt', 'opts.RunParallelExpts must be a logical or a number of cores.')
    end
    
    % Create parpool if it doesn't already exist
    p = gcp('nocreate');
    if isempty(p)
        p = parpool(nwMax);
    end
    nw = p.NumWorkers;
end

if opts.RunParallelExpts
    % Split experiments into worker groups
    componentMaps = splitList(opts.fit.ComponentMap, nw);
    
    % Generate objective function parts for each worker
    slave_objectives = cell(1,nw);
    for iw = 1:nw
        slave_objectives{iw} = generateSlaveObjective(m, con, obj, opts, componentMaps{iw});
    end
    
    % Distribute slave objective functions to workers
    spmd
        slave_objectives = codistributed(slave_objectives);
    end
    
    fminconObjective = @parallelExperimentObjective;
    
else
    
    fminconObjective = @serialObjective;
    
end

for iRestart = 1:opts.Restart+1
    % Init abort parameters
    aborted = false;
    Tabort = That;
    
    % Create local optimization problem
    %   Always needed - used as a subset/refinement of global optimization
    localProblem = createOptimProblem('fmincon', 'objective', fminconObjective, ...
        'x0', That, 'Aeq', opts.Aeq, 'beq', opts.beq, ...
        'lb', LowerBound, 'ub', UpperBound, 'options', localOpts);
    
    % Run specified optimization
    if opts.GlobalOptimization
        
        if opts.Verbose
            fprintf('Beginning global optimization with %s...\n', globalOpts.Algorithm)
        end
        
        switch globalOpts.Algorithm
            case 'globalsearch'
                gs = GlobalSearch('StartPointsToRun', globalOpts.StartPointsToRun);
                [That, G, exitflag] = run(gs, localProblem);
            case 'multistart'
                ms = MultiStart('StartPointsToRun', globalOpts.StartPointsToRun, ...
                    'UseParallel', globalOpts.UseParallel);
                [That, G, exitflag] = run(ms, localProblem, globalOpts.nStartPoints);
            case 'patternsearch'
                psOpts = psoptimset('MaxIter', globalOpts.MaxIter, 'UseParallel', globalOpts.UseParallel);
                [That, G, exitflag] = patternsearch(fminconObjective, That, [], [], ...
                    opts.Aeq, opts.beq, LowerBound, UpperBound, [], psOpts);
            otherwise
                error('Error:KroneckerBio:FitObjective: %s global optimization algorithm not recognized.', globalOpts.Algorithm)
        end
        
        [~, D] = fminconObjective(That); % since global solvers don't return gradient at endpoint
        
    else
        if strcmp(opts.Method, 'fmincon')
            if opts.Verbose; fprintf('Beginning gradient descent...\n'); end
            [That, G, exitflag, ~, ~, D] = fmincon(localProblem);
        end
        
    end
    
    % Check abortion status
    % Abortion values are not returned by fmincon and must be retrieved
    if aborted
        That = Tabort;
        [G, D] = fminconObjective(That);
    end
    
    % Re-apply stiff bounds
    That(That < LowerBound) = LowerBound(That < LowerBound);
    That(That > UpperBound) = UpperBound(That > UpperBound);
    
    % See if these parameters are better than previous iterations
    if G < Gbest
        Gbest = G;
        Tbest = That;
    end

    % Terminate if goal has been met
    if G <= opts.TerminalObj
        break
    end
    
    % Jump parameters before restarting
    % Retain the deterministic nature of fitting by fixing the stream
    if iRestart < opts.Restart + 1
        rng_state = rng;
        That = inf2big(That); % fix -Infs (can occur if normalized and got 0)
        prodThat = prod(That); % model fingerprint
        rng(mod(prodThat/eps(prodThat)*iRestart,2^32));
        if opts.Normalized
            That = Tbest + randn(nT,1) .* vec(opts.RestartJump(iRestart,G));
        else
            That = exp(log(Tbest) + randn(nT,1) .* vec(opts.RestartJump(iRestart,G)));
        end
        rng(rng_state);
        
        % Prevent jumps from leaving bounds
        while any(That < LowerBound) || any(That > UpperBound)
            That(That < LowerBound) = 2*(LowerBound(That < LowerBound)) - That(That < LowerBound);
            That(That > UpperBound) = 2*(UpperBound(That > UpperBound)) - That(That > UpperBound);
        end
    end
end

% Unnormalize
if opts.Normalized
    Tbest = exp(Tbest);
end

% Update parameter sets
[m, con] = updateAllActiveParameters(m, con, Tbest, opts.fit.ComponentMap, opts.fit.T2Tlocal);

% End of function
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Halt optimization on terminal goal %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    function stop = isTerminalObj(x, optimValues, state)
        if optimValues.fval < opts.TerminalObj
            aborted = true;
            Tabort = x;
            stop = true;
        else
            stop = false;
        end
    end

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Serial experiment objective function %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    function [G, D] = serialObjective(T)
        
        % Unnormalize
        if opts.Normalized
            T = exp(T);
        else
            % If fmincon chooses values that violate the lower bounds, force them to be equal to the lower bounds
            T(T < LowerBound) = LowerBound(T < LowerBound);
        end
        
        % Update parameter sets
        [m, con] = updateAllActiveParameters(m, con, T, opts.fit.ComponentMap, opts.fit.T2Tlocal);
        
        % Integrate system to get obj val, grad, etc.
        if nargout == 1
            G = computeObjAll(m, con, obj, opts, 'val');
        end
        if nargout == 2
            [G, D] = computeObjAll(m, con, obj, opts, 'grad');
        end
    end

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Master parallel experiment objective function %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    function [G,D] = parallelExperimentObjective(T)
        
        if nargout < 2
            
            spmd
                this_slave_objective = getLocalPart(slave_objectives);
                G_d = this_slave_objective{1}(T);
            end
            
        else
            
            spmd
                this_slave_objective = getLocalPart(slave_objectives);
                [G_d, D_d] = this_slave_objective{1}(T);
            end
           
            % Sum slave objective function gradients to get total gradient
            D = zeros(numel(T),1);
            for jw = 1:nw
                D = D + D_d{jw};
            end
            
        end
        
        % Sum slave objective function values to get total value
        G = 0;
        for jw = 1:nw
            G = G + G_d{jw};
        end
        
    end

end

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Slave objective function generator %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function objectivefun = generateSlaveObjective(m, con, obj, opts, ComponentMap)
% T_experiment:
%   nT-by-1 vector of experiment indices indicating which experiment fits
%   each parameter. 0 indicates model parameter fit by all experiments.
% i_cons:
%   Vector of experimental indices to be simulated by the slave function.

% If worker has no experiments assigned, assign a 0-valued objective
% function to it
if isempty(ComponentMap)
    clear m con obj opts componentMap % to avoid wasting memory in closure
    objectivefun = @emptyobjective;
    return
end

% Build options struct for the experiments on this worker
opts_iw = BuildFitOpts;
opts_iw = BuildFitOpts(opts_iw, opts);
opts_iw.fit = []; % prep for re-adding just the desired conditions

nCon = size(ComponentMap,1);
for iCon = 1:nCon
    conInd = ComponentMap{iCon,2};
    conopts = [];
    conopts.UseParams = opts.fit.UseParams{conInd};
    conopts.UseSeeds = opts.fit.UseSeeds{conInd};
    conopts.UseInputControls = opts.fit.UseInputControls{conInd};
    conopts.UseDoseControls = opts.fit.UseDoseControls{conInd};
    conopts.LowerBounds = opts.fit.LowerBounds{conInd};
    conopts.UpperBounds = opts.fit.UpperBounds{conInd};
    conopts.Continuous = opts.fit.Continuous(conInd);
    conopts.Complex = opts.fit.Complex(conInd);
    conopts.tGet = opts.fit.tGet{conInd};
    conopts.AbsTol = opts.fit.AbsTol{conInd};
    conopts.ObjWeights = opts.fit.ObjWeights{conInd};
    opts_iw = BuildFitOpts(opts_iw, m(ComponentMap{iCon,1}), con(conInd), obj([ComponentMap{iCon,3}]), conopts); %%TODO%% rebuilding BuildFitOpts gives a different overall T vector
end

% Make param mapper functions that map the right subset of params
opts_iw.fit.ParamMapper = opts.fit.ParamMapper;
opts_iw.fit.Tlocal2T = @Tlocal2T_iw;
opts_iw.fit.T2Tlocal = @T2Tlocal_iw;
    function T = Tlocal2T_iw(Tlocal, mode)
        if nargin < 2
            mode = 'overwrite';
        end
        Tlocal_all = opts.fit.T2Tlocal(zeros(opts.fit.ParamMapper.nT,1)); % Generate dummy empty Tlocal on which to paste this worker's Tlocal
        Tlocal_all([ComponentMap{:,2}]) = Tlocal;
        T = opts.fit.Tlocal2T(Tlocal_all, mode);
    end
    function Tlocal = T2Tlocal_iw(T)
        Tlocal_all = opts.fit.T2Tlocal(T);
        Tlocal = Tlocal_all([ComponentMap{:,2}]);
    end

% Collect bounds
LowerBound = opts_iw.fit.Tlocal2T(opts_iw.fit.LowerBounds);
if opts_iw.Normalized
    LowerBound = log(LowerBound);
end

% Keep the components specified in componentMap
m = m([ComponentMap{:,1}]);
con = con([ComponentMap{:,2}]);
obj = obj(vertcat(ComponentMap{:,3}));

% Return worker-specific objective function
objectivefun = @objective;

    function [G, D] = objective(T)
        % Slave objective function
        
        % Reset answers
        G = 0;
        D = zeros(length(T),1);
        
        % Unnormalize
        if opts_iw.Normalized
            T = exp(T);
        else
            % If fmincon chooses values that violate the lower bounds, force them to be equal to the lower bounds
            T(T < LowerBound) = LowerBound(T < LowerBound);
        end
        
        % Update parameter sets
        [m, con] = updateAllActiveParameters(m, con, T, opts_iw.fit.ComponentMap, opts_iw.fit.T2Tlocal);
        
        % Integrate system to get obj val, grad, etc.
        if nargout == 1
            G = computeObjAll(m, con, obj, opts_iw, 'val');
        end
        if nargout == 2
            [G, D] = computeObjAll(m, con, obj, opts_iw, 'grad');
        end
    end

    function [G, D] = emptyobjective(T)
        % Function to use if worker has no experiments assigned to it
        G = 0;
        D = zeros(length(T),1);
    end

end

%% Helper functions
function validateOpts(m, con, obj, opts)
% Make sure models+conditions+objectives match (in number at least) the options.
% Throws an exception if invalid
assert(isfield(opts, 'fit'), 'FitObjective:validateOpts:MissingExperimentOpts', 'Missing experiment-specific options in opts.fit field.')

componentMap = opts.fit.ComponentMap;

nModels = length(m);
nModelComponents = length(unique([componentMap{:,1}]));
assert(nModels == nModelComponents, 'FitObjective:validateOpts:ModelCountMismatch', '%g models in m but %g models in opts', nModels, nModelComponents);

nCon = length(con);
nConComponents = length(unique([componentMap{:,2}]));
assert(nCon == nConComponents, 'FitObjective:validateOpts:ConditionCountMismatch', '%g conditions in con but %g conditions in opts', nCon, nConComponents);

nObj = length(obj);
nObjComponents = length(unique([componentMap{:,3}]));
assert(nObj == nObjComponents, 'FitObjective:validateOpts:ObjectiveCountMismatch', '%g objectives in obj but %g objectives in opts', nObj, nObjComponents);
end
