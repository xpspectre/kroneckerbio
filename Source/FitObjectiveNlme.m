function [m, con, G, D] = FitObjectiveNlme(m, con, obj, opts)
%FitObjectiveNlme Optimize the Theta, Omega, and Sigma of a population nonlinear
%mixed effects (NLME) model
%
% Usage:
%   1. Original way, 1st set of input args below. Single model with a
%      vector of conditions and matrix of objective functions, where all
%      conditions are derived from the model, and each row of objective
%      functions is derived from the corresponding condition row.
%	2. New way, 2nd set of input args below. Multiple models,
%      conditions, and objective function vectors, plus an options struct
%      generated by BuildFitOpts to create the desired mapping.
%
%   FitObjective uses the derivatives in the Kronecker model and in the
%   objective functions to build a function that can not only evaluate the
%   objective functions at particular parameter sets, but also evaluate the
%   gradient at those parameter sets, thereby pointing in the direction of
%   a more optimum parameter set. This function is built around Matlab's
%   fmincon, which is a gradient descent minimizer. It varies the
%   parameters attempting to find the parameter set that will minimize the
%   objective function while keeping with the bounds.
%
%   The formulation is based on Almquist, J., Leander, J., & Jirstrand, M.
%       (2015). Using sensitivity equations for computing gradients of the
%       FOCE and FOCEI approximations to the population likelihood. Journal
%       of Pharmacokinetics and Pharmacodynamics, 191â€“209. doi:10.1007/s10928-015-9409-1
%
%   The NLME problem finds the optimal values of the fixed effects
%   parameters {Theta, Omega, Sigma} while accounting for random effects
%   parameters Eta. Theta are the population average values, Omega is the
%   covariance matrix of the random effects parameters, which have a
%   normal(0,Omega) distribution, and Sigma is the covariance matrix of the
%   "measurement" error terms, which have a normal(0,Rij) distribution with
%   terms in Rij including sigma.
%
% Inputs:
%	m [ model struct scalar | nModel x 1 model struct vector ]
%       The KroneckerBio model that will be simulated
% 	con [ nCon x 1 experiment struct vector | nCon x 1 experiment struct vector ]
%       The experimental conditions under which the model will be simulated
%	obj [ nCon x nCon objective struct matrix | nObj x 1 objective struct vector ]
%       The objective structures defining the objective functions to be
%       evaluated.
%	opts [ options struct scalar ]
%       Options struct with just global opts or detailed options from
%       BuildFitOpts.
%
% Outputs:
%	m [ model struct scalar | nModel x 1 model struct vector ]
%       The model(s) with optimal rate parameters applied. If fitting parameters
%       that vary betwen experimental conditions, extra models are returned
%       (correspondence between m and con are in opts.fit.ComponentMap) to allow
%       immediate simulation of fits.
%	con [ nCon x 1 experiment struct vector | nCon x 1 experiment struct vector ]
%       The experimental conditions with optimal seed, input control, and
%       dose control parameters applied
%	G [ real scalar ]
%       The optimum objective function value
%	D [ real vector nT ]
%       The objective gradient at the optimum parameter set
%
%   Notes: 
%       - Global optimization has been disabled for now
%       - Parameter normalization doesn't work for now
%       - Requires forward sensitivity calculation (adjoint doesn't work
%       for now) - adjoint probably won't work because lots of different
%       types of sensitivities are needed
%       - Throughout, T refers to all params, and is what's fed into obj
%       fun calculators, Theta and Eta are separate and optimized on
%
%   TODO:
%       - Make a variable to keep track of etas, for better starting
%       guesses for inner optimization in FOCEI

%% Work-up
% Clean up inputs
if nargin < 4
    opts = [];
end
if nargin < 4 || ~isfield(opts, 'fit')
    % Convert Usage 1 -> Usage 2
    [m, con, obj, opts] = ConvertFitOpts(m, con, obj, opts);
end

validateOpts(m, con, obj, opts);

verbose = logical(opts.Verbose);
opts.Verbose = max(opts.Verbose-1,0);

% Ensure Restart is a positive integer
if ~(opts.Restart >= 0)
    opts.Restart = 0;
    warning('KroneckerBio:FitObjectiveNlme', 'opts.Restart was not nonegative. It has been set to 0.')
end

if ~(opts.Restart == floor(opts.Restart))
    opts.Restart = floor(opts.Restart);
    warning('KroneckerBio:FitObjectiveNlme', 'opts.Restart was not a whole number. It has been floored.')
end

% Ensure RestartJump is a function handle
if isnumeric(opts.RestartJump)
    opts.RestartJump = @(iter,G)(opts.RestartJump);
end

% Add dummy models if multiple conditions depend on independent k's
nCon = length(con);
addDummyModel = opts.fit.AddDummyModel;
for iCon = 1:nCon
    iDummy = addDummyModel(iCon);
    if iDummy % not 0
        % See if the required dummy model has already been added
        prevDummy = addDummyModel(1:iCon-1);
        if ismember(iDummy, prevDummy)
            iDummyNew = opts.fit.ComponentMap{ismember(prevDummy, iDummy),1}; % get reassigned model ind from previously added dummy
        else
            iBaseModel = opts.fit.ComponentMap{iCon,1};
            mName = m(iBaseModel).Name;
            mDummy = m(iBaseModel);
            m = [m; mDummy];
            iDummyNew = length(m);
            m(iDummyNew) = m(iDummyNew).UpdateField(struct('Name', [mName '_Dummy_' num2str(iDummyNew)]));
        end
        opts.fit.ComponentMap{iCon,1} = iDummyNew; % reassign component map to point to dummy model
    end
end

validateOpts(m, con, obj, opts);

% Collect bounds
LowerBound = opts.fit.Tlocal2T(opts.fit.LowerBounds);
UpperBound = opts.fit.Tlocal2T(opts.fit.UpperBounds);

% Assign starting values of params (default is values in model when opts was built)
[m, con] = updateAllActiveParameters(m, con, opts.fit.Tlocal2T(opts.fit.ParamStart), opts.fit.ComponentMap, opts.fit.T2Tlocal);

%% Construct starting variable parameter set
nT = length(LowerBound);
[T0, details] = collectAllActiveParameters(m, con, opts.fit.ComponentMap, opts.fit.Tlocal2T);
assert(length(T0) == nT);

[thetas, etas] = getParameterMask(details{:,'Name'});
Theta0 = T0(thetas);
nTheta = length(Theta0);

% Collect bounds for thetas only
LowerBound = LowerBound(thetas);
UpperBound = UpperBound(thetas);

% Apply bounds to starting parameters before optimizing
% fmincon will choose a weird value if a starting parameter is outside the bounds
Theta0(Theta0 < LowerBound) = LowerBound(Theta0 < LowerBound);
Theta0(Theta0 > UpperBound) = UpperBound(Theta0 > UpperBound);

% Whether to use conditional expectation
%   Enables inner optimization to find mode of eta
if strcmp(opts.Approximation, 'FOCEI')
    useCE = true;
else
    useCE = false;
end

% Number of participants
ni = length(con);

%% NLME-specific options
% Enable (n+1)-th order sensitivity calculation
opts.ComputeSensPlusOne = true;

% Disable parameter normalization
if opts.Normalized
    opts.Normalized = false;
    warning('KroneckerBio:FitObjectiveNlme:NormalizationDisabled', 'FitObjectiveNlme doesn''t currently support normalized parameters. opts.Normalized set to false.')
end

% Disable adjoint sensitivity calculation
if opts.UseAdjoint
   opts.UseAdjoint = false;
   warning('KroneckerBio:FitObjectiveNlme:AdjointSensitivityDisabled', 'FitObjectiveNlme doesn''t currently support adjoint sensitivity calculation. opts.UseAdjoint set to false.')
end

% Disable complex step differentiation
if opts.ComplexStep
    opts.ComplexStep = false;
    warning('KroneckerBio:FitObjectiveNlme:ComplexStepDifferentiationDisabled', 'FitObjectiveNlme doesn''t currently support complex step differentiation. opts.ComplexStep set to false.')
end

%% Outer (theta) optimization options
% Default options
outerOpts_ = [];
outerOpts_.Algorithm               = 'active-set';
outerOpts_.UseFiniteDifferences    = false;
outerOpts_.TolOptim                = 1e-4;
outerOpts_.TolX                    = 0;
outerOpts_.MaxFunEvals             = 20000; % more fun evals/step 
outerOpts_.MaxIter                 = 1000;
outerOpts_.MaxStepSize             = 0.1; % smaller
outerOpts_.RelLineSrchBndDuration  = Inf;
outerOpts_.TolCon                  = 1e-4; % larger
outerOpts_.DerivativeCheck         = 'off'; % Turn on to debug analytic gradient calculation
outerOpts_.FinDiffType             = 'central'; % Slower but more accurate

outerOpts_ = mergestruct(outerOpts_, opts);

if outerOpts_.UseFiniteDifferences
    outerOpts_.GradObj = 'off';
else
    outerOpts_.GradObj = 'on';
end

outerOpts = optimoptions('fmincon');
outerOpts.Algorithm               = outerOpts_.Algorithm;
outerOpts.TolFun                  = outerOpts_.TolOptim;
outerOpts.TolX                    = outerOpts_.TolX;
outerOpts.OutputFcn               = @isTerminalObjOuter;
outerOpts.GradObj                 = outerOpts_.GradObj;
outerOpts.Hessian                 = 'off';
outerOpts.MaxFunEvals             = outerOpts_.MaxFunEvals;
outerOpts.MaxIter                 = outerOpts_.MaxIter;
outerOpts.RelLineSrchBnd          = outerOpts_.MaxStepSize;
outerOpts.RelLineSrchBndDuration  = outerOpts_.RelLineSrchBndDuration;
outerOpts.TolCon                  = outerOpts_.TolCon;
outerOpts.DerivativeCheck         = outerOpts_.DerivativeCheck;
outerOpts.FinDiffType             = outerOpts_.FinDiffType;

if verbose > 1
    outerOpts.Display = 'iter'; % or 'iter-detailed'
else
    outerOpts.Display = 'off';
end

%% Abort if no fit params specified
if numel(Theta0) == 0
    [G, D] = outerObjective(Theta0);
    return
end

%% Run optimization
% Initialize loop
ThetaHat  = Theta0;
Gbest = Inf;
ThetaBest = Theta0;

for iRestart = 1:opts.Restart+1
    % Init abort parameters
    aborted = false;
    ThetaAbort = ThetaHat;
    
    % Create local optimization problem
    %   Always needed - used as a subset/refinement of global optimization
    localProblem = createOptimProblem('fmincon', 'objective', @outerObjective, ...
        'x0', ThetaHat, 'Aeq', opts.Aeq, 'beq', opts.beq, ...
        'lb', LowerBound, 'ub', UpperBound, 'options', outerOpts);
    
    if opts.Verbose; fprintf('Beginning gradient descent...\n'); end
    [ThetaHat, G, exitflag, ~, ~, D] = fmincon(localProblem);
    
    % Check abortion status
    % Abortion values are not returned by fmincon and must be retrieved
    if aborted
        ThetaHat = ThetaAbort;
        [G, D] = outerObjective(ThetaHat);
    end
    
    % Re-apply stiff bounds
    ThetaHat(ThetaHat < LowerBound) = LowerBound(ThetaHat < LowerBound);
    ThetaHat(ThetaHat > UpperBound) = UpperBound(ThetaHat > UpperBound);
    
    % See if these parameters are better than previous iterations
    if G < Gbest
        Gbest = G;
        ThetaBest = ThetaHat;
    end

    % Terminate if goal has been met
    if G <= opts.TerminalObj
        break
    end
    
    % Jump parameters before restarting
    % Retain the deterministic nature of fitting by fixing the stream
    rng_state = rng;
    ThetaHat = inf2big(ThetaHat); % fix -Infs (can occur if normalized and got 0)
    prodThat = prod(ThetaHat); % model fingerprint
    rng(mod(prodThat/eps(prodThat)*iRestart,2^32));
    ThetaHat = exp(log(ThetaBest) + randn(nTheta,1) .* vec(opts.RestartJump(iRestart,G)));
    rng(rng_state);
    
    % Prevent jumps from leaving bounds
    while any(ThetaHat < LowerBound) || any(ThetaHat > UpperBound)
        ThetaHat(ThetaHat < LowerBound) = 2*(LowerBound(ThetaHat < LowerBound)) - ThetaHat(ThetaHat < LowerBound);
        ThetaHat(ThetaHat > UpperBound) = 2*(UpperBound(ThetaHat > UpperBound)) - ThetaHat(ThetaHat > UpperBound);
    end
end

% Update parameter sets
%   etas kept at 0 - for FO - fill them in for individuals later as desired
T = zeros(nT,1);
T(thetas) = ThetaBest;
[m, con] = updateAllActiveParameters(m, con, T, opts.fit.ComponentMap, opts.fit.T2Tlocal);

% End of function
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Outer (theta) objective function %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    function [G, D] = outerObjective(Theta)
        % Objective function optimizes on vector or thetas.
        % Note: Thetas are put into the total vector of all params for
        %   computeObjective. Gradient is return w.r.t. thetas only.
        
        % If fmincon chooses values outside the bounds, force them to
        % equal to the bound
        Theta(Theta < LowerBound) = LowerBound(Theta < LowerBound);
        Theta(Theta > UpperBound) = UpperBound(Theta > UpperBound);
        
        %% Update parameter sets
        T = zeros(nT,1);
        T(thetas) = Theta;
        
        % Perform inner optimization for FOCEI
        % Calculate obj fun val and grad using optimal etas for each
        %   individual
        if useCE % FOCEI
            fprintf('Performing inner optimization...\n') % DEBUG
            
            if nargout == 1
                G = 0;
                for i = 1:ni
                    Gi = calcIndividualContribution(i);
                    G = G + Gi;
                end
            end
            if nargout == 2
                G = 0;
                D = zeros(nTheta,1);
                for i = 1:ni
                    [Gi, Di] = calcIndividualContribution(i);
                    G = G + Gi;
                    D = D + Di;
                end
            end
            
        else % FO
            [m, con] = updateAllActiveParameters(m, con, T, opts.fit.ComponentMap, opts.fit.T2Tlocal);
            
            % Integrate system to get obj val, grad, etc.
            if nargout == 1
                G = computeObjAll(m, con, obj, opts, 'val');
            end
            if nargout == 2
                [G, D] = computeObjAll(m, con, obj, opts, 'grad');
                D = D(thetas);
            end
        end
        
    end

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Halt outer optimization on terminal goal %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    function stop = isTerminalObjOuter(x, optimValues, state)
        if optimValues.fval < opts.TerminalObj
            aborted = true;
            ThetaAbort = x;
            stop = true;
        else
            stop = false;
        end
    end

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Calculate individual contribution in FOCEI %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    function [G, D] = calcIndividualContribution(i)
        
        error('FOCEI individual contributions have not been updated')
        
        % Create inner fitting problem
%         innerFit = FitObject(['Fit_FOCEI_Inner_', num2str(i)]);
%         innerFit.addModel(fit.Models(1));
%         innerObs = observationIndividualVariability(fit.Objectives(i).private.outputs, fit.Objectives(i).private.times);
%         innerObj = innerObs.Objective(fit.Objectives(i).private.measurements);
%         innerFit.addFitConditionData(innerObj, fit.Conditions(i));
        
        % Run inner fitting problem
        [~, ~, ~, eta_i] = fitIndividualVariability(innerFit, opts);
        
        % Calculate obj fun val and grad cont
        T_i = T;
        T_i(etas) = eta_i;
        fit.updateParams(T_i); % temporarily update fit to the individual's optimal eta
        
        if nargout == 1
            G = fit.computeObjective(i);
        end
        if nargout == 2
            [G, D] = fit.computeObjective(i);
            D = D(thetas);
        end
        
        % Reset fit's params - is this inefficient?
        fit.updateParams(T);
        
    end
end

%% Helper functions
function validateOpts(m, con, obj, opts)
% Make sure models+conditions+objectives match (in number at least) the options.
% Throws an exception if invalid
assert(isfield(opts, 'fit'), 'FitObjective:validateOpts:MissingExperimentOpts', 'Missing experiment-specific options in opts.fit field.')

componentMap = opts.fit.ComponentMap;

nModels = length(m);
nModelComponents = length(unique([componentMap{:,1}]));
assert(nModels == nModelComponents, 'FitObjective:validateOpts:ModelCountMismatch', '%g models in m but %g models in opts', nModels, nModelComponents);

nCon = length(con);
nConComponents = length(unique([componentMap{:,2}]));
assert(nCon == nConComponents, 'FitObjective:validateOpts:ConditionCountMismatch', '%g conditions in con but %g conditions in opts', nCon, nConComponents);

nObj = length(obj);
nObjComponents = length(unique([componentMap{:,3}]));
assert(nObj == nObjComponents, 'FitObjective:validateOpts:ObjectiveCountMismatch', '%g objectives in obj but %g objectives in opts', nObj, nObjComponents);
end
